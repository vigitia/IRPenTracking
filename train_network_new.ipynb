{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd9f5f5-07f0-4ed2-9576-35193b761582",
   "metadata": {},
   "source": [
    "# TipTrack CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee560ba-f876-4d26-9c89-49a62e042970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import categorical_crossentropy, binary_crossentropy\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, RandomFlip, RandomRotation\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import Callback\n",
    "import visualkeras\n",
    "# from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomBrightness\n",
    "\n",
    "# Custom python class for preparing the dataset. This will prevent the notebook from being too messy\n",
    "from prepare_dataset import PrepareDataset\n",
    "\n",
    "# Ignore CPU warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc373a28-fe90-4d21-88d6-df3d3f9a50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 48  # The expected size of the input images\n",
    "TARGET_FOLDER = 'training_images/2023-02-24'  # The folder containing the new images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a6651",
   "metadata": {},
   "source": [
    "## Prepare the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f8a25-e684-4c95-9a61-f87934ad8ed7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prepare_dataset = PrepareDataset()\n",
    "\n",
    "train_X, train_label, test_X, test_label = prepare_dataset.get_dataset(TARGET_FOLDER, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89989190-af7d-4571-a2c0-9341ce3232ae",
   "metadata": {},
   "source": [
    "## Init Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3201b",
   "metadata": {},
   "source": [
    "**Breakdown of the current architecture:**\n",
    "\n",
    "The model is defined as a Sequential object, meaning layers will be added in a sequential manner.\n",
    "\n",
    "The first layer added is a Conv2D layer with 64 filters, each with a kernel size of 3x1, and a linear activation function. The input shape is (48, 48, 1), which means the input is a grayscale image with dimensions 48x48. The padding is set to 'same', which means the output will have the same spatial dimensions as the input.\n",
    "\n",
    "The next layer is another Conv2D layer with 64 filters, this time with a kernel size of 1x3 and also with a linear activation function. The padding is set to 'same'.\n",
    "\n",
    "A MaxPooling2D layer is added with a pool size of 2x2 and padding set to 'same'. This layer reduces the spatial dimensions of the output by a factor of 2.\n",
    "\n",
    "Two more pairs of Conv2D and MaxPooling2D layers are added, each with a lower number of filters (32) but larger kernel sizes (3x1 and 1x3). The padding is set to 'same' for both convolutional layers, and the pool size is again 2x2 with padding set to 'same' for both pooling layers. A Dropout layer with rate 0.2 is added after the second pair of convolutional and pooling layers.\n",
    "\n",
    "A Flatten layer is added to flatten the output of the previous layer into a 1D array.\n",
    "\n",
    "Four Dense layers with 128 units each and a ReLU activation function are added in sequence.\n",
    "\n",
    "Another Dropout layer with rate 0.2 is added after the fifth Dense layer.\n",
    "\n",
    "A final Dense layer with 2 units (one for each possible class) and a softmax activation function is added.\n",
    "\n",
    "The model is compiled using categorical_crossentropy as the loss function, the Adam optimizer, and accuracy as the metric to monitor during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fd49c-536a-4b51-88e8-3cd2357085a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def init_keras_model():\n",
    "    # conv linear\n",
    "    activation = 'relu'\n",
    "    activation_conv = 'LeakyReLU'  # LeakyReLU\n",
    "    num_neurons = 64 # 128\n",
    "\n",
    "    # Define model structure\n",
    "    model = Sequential()\n",
    "\n",
    "    # model.add(data_augmentation_new)\n",
    "    model.add(Conv2D(64, kernel_size=(3, 1), activation=activation_conv, input_shape=(IMG_SIZE, IMG_SIZE, 1), padding='same'))\n",
    "    model.add(Conv2D(64, (1, 3), activation=activation_conv, padding='same'))\n",
    "\n",
    "    # Reduce the spatial dimensions of the output by a factor of 2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 1), activation=activation_conv, padding='same'))\n",
    "    model.add(Conv2D(32, (1, 3), activation=activation_conv, padding='same'))\n",
    "\n",
    "    # Reduce the spatial dimensions of the output by a factor of 2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    #model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation=activation_conv, padding='same'))\n",
    "    # Reduce the spatial dimensions of the output by a factor of 2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # trial and error: linear performs much better than ReLU and sigmoid\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "    # model.add(Dense(num_neurons, activation=activation))\n",
    "    # model.add(Dense(num_neurons, activation=activation))\n",
    "    # model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "    # model.add(Dense(64, activation=activation))\n",
    "    # model.add(Dense(64, activation=activation))\n",
    "    # model.add(Dense(128, activation='linear'))\n",
    "    # model.add(Dense(128, activation='linear'))\n",
    "\n",
    "    # Classifier\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # plot_model(model)\n",
    "    visualkeras.layered_view(model, legend=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bea4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mayankverma05032001/binary-classification-using-convolution-neural-network-cnn-model-6e35cdf5bdbb\n",
    "\n",
    "def init_keras_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size=(3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "    \n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128,kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # TODO: Why here 2 and not 1 for binary classification?\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=binary_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # plot_model(model)\n",
    "    visualkeras.layered_view(model, legend=True)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db61fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_keras_model_2():\n",
    "#     model = Sequential()\n",
    "#         # Note the input shape is the desired size of the image 200x200 with 3 bytes color\n",
    "#         # This is the first convolution\n",
    "#     model.add(Conv2D(16, (3,3), activation='relu', input_shape=(48, 48, 1)))\n",
    "#     model.add(MaxPooling2D(2, 2))\n",
    "#         # The second convolution\n",
    "#     model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(2,2))\n",
    "#         # The third convolution\n",
    "#     model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(2,2))\n",
    "#         # The fourth convolution\n",
    "#     model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(2,2))\n",
    "#         # # The fifth convolution\n",
    "#     model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(2,2))\n",
    "#         # Flatten the results to feed into a DNN\n",
    "#     model.add(Flatten())\n",
    "#         # 512 neuron hidden layer\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "#         # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('dandelions') and 1 for the other ('grass')\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "#     model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001), metrics='accuracy')\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2902888-99f8-4ce9-940a-4149e18dc58a",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdef552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Keras callback class to plot the data after each epoch\n",
    "class plot_callback(Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global history\n",
    "        global total_epochs\n",
    "        global loss\n",
    "        global val_loss\n",
    "        global accuracy\n",
    "        global val_accuracy\n",
    "        \n",
    "        total_epochs += 1\n",
    "        \n",
    "        loss.append(logs.get('loss'))\n",
    "        val_loss.append(logs.get('val_loss'))\n",
    "        accuracy.append(logs.get('accuracy'))\n",
    "        val_accuracy.append(logs.get('val_accuracy'))\n",
    "        \n",
    "        # if total_epochs > 1:\n",
    "            \n",
    "        plt.plot(accuracy, 'blue', label='Training accuracy', marker=\"o\", markersize=5)\n",
    "        plt.plot(val_accuracy, 'green', label='Validation accuracy', marker=\"o\", markersize=5)\n",
    "        plt.plot(loss, 'red', label='Loss', marker=\"o\", markersize=5)\n",
    "        plt.plot(val_loss, 'orange', label='Validation loss', marker=\"o\", markersize=5)\n",
    "\n",
    "        plt.title('Training and Validation accuracy/loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy/Loss')\n",
    "        plt.xticks(np.arange(0, total_epochs + 1, 1))\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "        #plt.title('Training and Validation loss')\n",
    "        #plt.xlabel('Epochs')\n",
    "        #plt.ylabel('Loss')\n",
    "        #plt.xticks(np.arange(0, total_epochs + 1, 1))\n",
    "        #plt.ylim(0, 1)\n",
    "        #plt.legend()\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc511c-b2ac-41c8-a170-9f60ea151ab2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_epochs= 0\n",
    "loss = []\n",
    "val_loss = []\n",
    "accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "model = init_keras_model_2()\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_X,\n",
    "    y=train_label,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    shuffle=True,  # Try shuffling data here after each epoch\n",
    "    validation_data=(test_X, test_label),\n",
    "    callbacks=[plot_callback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1233ca-e0e7-48e3-81ec-ec84c26af333",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40feae9e-ef74-49c9-b7a0-4ea217b1a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_2023_019'  # How the output model should be named at the end\n",
    "\n",
    "if MODEL_NAME in next(os.walk('.'))[1]:\n",
    "    print('Warning: There already exists a model called \"{}\"'.format(MODEL_NAME))\n",
    "    print('Make sure you want to overwrite it before it gets saved!')\n",
    "else:\n",
    "    model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba52bf",
   "metadata": {},
   "source": [
    "### Possible problems (ChatGPT)\n",
    "\n",
    "Without knowing the specifics of the dataset and the task at hand, it's difficult to say whether this model is appropriate or not. However, there are some general points to consider:\n",
    "\n",
    "1.  The model has a relatively large number of parameters, which can increase the risk of overfitting on small datasets. In particular, having five consecutive Dense layers with 128 units each may be unnecessary for some tasks and can lead to overfitting if the dataset is not large enough.\n",
    "2.  The use of linear activation functions in the convolutional layers is not common in modern architectures. In most cases, rectified linear unit (ReLU) or its variants are used as activation functions in convolutional layers. However, it's possible that linear activation functions are appropriate for the specific task at hand.\n",
    "3.  The Dropout rate of 0.2 used in this model may be too low for some datasets, especially if the model is prone to overfitting. In general, it's recommended to start with a higher rate of Dropout (e.g. 0.5) and then tune it based on the performance of the model on the validation set.\n",
    "4.  There is no use of Batch Normalization, which is a technique used to improve the training of deep neural networks. Batch Normalization can help improve the stability of the network during training and improve the overall accuracy of the model. However, it's possible that the specific task at hand does not require Batch Normalization.\n",
    "5.  Lastly, it's important to ensure that the dataset is properly preprocessed before training the model. This includes tasks such as scaling the pixel values to a common range, converting images to grayscale or RGB, and splitting the dataset into training, validation, and test sets.\n",
    "\n",
    "To improve the prediction of a neural network model, here are some suggestions that you can try:\n",
    "\n",
    "1.  Adjust the model architecture: Try adding or removing layers, adjusting the number of neurons, or changing the activation functions. For example, you could try using ReLU activation functions in the convolutional layers instead of linear, or reducing the number of neurons in the dense layers to prevent overfitting.\n",
    "2.  Increase the amount of data: If you have access to more data, you can try increasing the size of your training dataset, which can help improve the model's performance. Alternatively, you could try data augmentation techniques such as rotating, flipping, or shifting the images to generate more training examples.\n",
    "3.  Tune hyperparameters: Experiment with different hyperparameters such as learning rate, batch size, and regularization rate to see if you can find a combination that works better for your dataset. You can use techniques such as grid search or random search to automate this process.\n",
    "4.  Use pre-trained models: You can try using pre-trained models such as VGG, ResNet, or Inception to leverage the knowledge gained from training on large datasets like ImageNet. By fine-tuning the last few layers of the pre-trained model, you can adapt it to your specific task and achieve better performance.\n",
    "5.  Analyze the errors: Analyze the errors made by the model to identify common patterns or types of images that are difficult to classify. This can help guide you towards adjustments to the model architecture or dataset that may improve performance on these difficult cases.\n",
    "\n",
    "It's important to note that improving the performance of a machine learning model can be an iterative process, and you may need to try multiple strategies before achieving the desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb17d92",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    dense_layer_count = hp.Int('dense_layer_count', min_value=1, max_value=5, step=1)\n",
    "    dense_layer_neurons = hp.Int('dense_layer_neurons', min_value=32, max_value=128, step=32)\n",
    "    dense_layer_activation = hp.Choice('dense_layer_activation', values=['linear', 'sigmoid', 'relu'])\n",
    "    conv_layer_count = hp.Int('conv_layer_count', min_value=1, max_value=5, step=1)\n",
    "    conv_layer_activation = hp.Choice('conv_layer_activation', values=['linear', 'sigmoid', 'relu'])\n",
    "    conv_kernel_split = hp.Choice('conv_kernel_split', values=[True, False])\n",
    "    conv_kernel_size = hp.Int('conv_kernel_size', min_value=3, max_value=7, step=2)\n",
    "    conv_kernel_stride = hp.Choice('conv_stride', values=[1, 2, 3, 4, 5])\n",
    "    conv_kernel_count = hp.Int('conv_kernel_count', min_value=16, max_value=64, step=16)\n",
    "    #learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    optimizer = hp.Choice('optimizer', values=['Adam', 'Adamax', 'Nadam'])\n",
    "    conv_dropout = hp.Choice('conv_dropout', values=[True, False])\n",
    "    dense_dropout = hp.Choice('dense_dropout', values=[True, False])\n",
    "\n",
    "    #for epochs in range(1, 7):\n",
    "        #print(f'+++++ EPOCHS: {epochs} +++++')\n",
    "    #Define model structure\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    stride = (conv_kernel_stride, conv_kernel_stride)\n",
    "    \n",
    "    #model.add(data_augmentation_new)\n",
    "    if conv_kernel_split:\n",
    "        model.add(Conv2D(conv_kernel_count, kernel_size=(conv_kernel_size, 1), activation=conv_layer_activation, input_shape=(IMG_SIZE,IMG_SIZE,1), padding='same', strides=stride))\n",
    "        model.add(Conv2D(conv_kernel_count, (1, conv_kernel_size), activation=conv_layer_activation, padding='same', strides=stride))\n",
    "    else:\n",
    "        model.add(Conv2D(conv_kernel_count, kernel_size=(conv_kernel_size, conv_kernel_size), activation=conv_layer_activation, input_shape=(IMG_SIZE,IMG_SIZE,1), padding='same', strides=stride))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    \n",
    "    for i in range(0, conv_layer_count - 1):\n",
    "        if conv_kernel_split:\n",
    "            model.add(Conv2D(conv_kernel_count, kernel_size=(conv_kernel_size, 1), activation=conv_layer_activation, padding='same', strides=stride))\n",
    "            model.add(Conv2D(conv_kernel_count, (1, conv_kernel_size), activation=conv_layer_activation, padding='same', strides=stride))\n",
    "        else:\n",
    "            model.add(Conv2D(conv_kernel_count, kernel_size=(conv_kernel_size, conv_kernel_size), activation=conv_layer_activation, padding='same', strides=stride))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    \n",
    "    if conv_dropout:\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # trial and error: linear performs much better than ReLU and sigmoid\n",
    "    for i in range(dense_layer_count):\n",
    "        if i == dense_layer_count - 1 and dense_layer_count > 1:\n",
    "            if dense_dropout:\n",
    "                model.add(Dropout(0.2))\n",
    "        model.add(Dense(dense_layer_neurons, activation=dense_layer_activation))\n",
    "\n",
    "    # classifier\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    #optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "    #optimizer = keras.optimizers.Adamax(learning_rate = 0.002, beta_1 = 0.9, beta_2 = 0.999)\n",
    "\n",
    "    model.compile(loss=tf.keras.metrics.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001) # patience 1, min_lr 0.00005\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=30\n",
    ")\n",
    "\n",
    "history = tuner.search(train_X, train_label, batch_size=128, epochs=15, validation_data=(valid_X, valid_label), callbacks=[reduce_lr, stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1f9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
